{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-10T13:57:08.811373Z",
     "start_time": "2025-07-10T13:56:58.790615Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from embeds import multi_inner_align, standardize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pickle\n",
    "import sys\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "from rca.rca import run_rca"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T13:57:10.036231Z",
     "start_time": "2025-07-10T13:57:08.828329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "norms_voc = set(\n",
    "    pd.read_csv('../../data/psychNorms/psychNorms.zip', index_col=0, low_memory=False, compression='zip').index\n",
    ")\n",
    "with open('../../data/brain_behav_union.pkl', 'rb') as f:\n",
    "    brain_behav_union = pickle.load(f)\n",
    "\n",
    "# Extract intersection of norms and brain_behavior_union\n",
    "to_extract = list(norms_voc & brain_behav_union)\n",
    "len(to_extract)"
   ],
   "id": "1d611baad73556f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46246"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extracting representations",
   "id": "84217b142e1e7c9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T13:57:15.882628Z",
     "start_time": "2025-07-10T13:57:10.349487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.random.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('CUDA is available. Using GPU.')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available. Using Apple's Metal.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU or MPS available. Using CPU.\")\n",
    "\n",
    "\n",
    "model_name = 'meta-llama/Llama-3.2-1B'\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Model\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ],
   "id": "1d4371fed19c32fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available. Using Apple's Metal.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T14:00:45.236884Z",
     "start_time": "2025-07-10T13:57:54.433777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 16\n",
    "embeds = {}\n",
    "with torch.no_grad():\n",
    "    # Loop through the data in chunks of `batch_size`\n",
    "    for i in tqdm(range(0, len(to_extract), batch_size)):\n",
    "        # Create a batch from the large list\n",
    "        batch_words = to_extract[i:i+batch_size]\n",
    "\n",
    "        # --- The rest of the logic is the same, just applied to `batch_words` ---\n",
    "        inputs = tokenizer(batch_words, return_tensors='pt', padding=True, truncation=True)\n",
    "        all_word_ids = [inputs.word_ids(j) for j in range(len(batch_words))]\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state.cpu()\n",
    "\n",
    "        for k, word in enumerate(batch_words):\n",
    "            word_ids = all_word_ids[k]\n",
    "            word_token_indices = [j for j, wid in enumerate(word_ids) if wid is not None]\n",
    "\n",
    "            word_hidden_states = last_hidden_state[k, word_token_indices, :]\n",
    "            averaged_word_representation = torch.mean(word_hidden_states, dim=0)\n",
    "\n",
    "            embeds[word] = averaged_word_representation\n",
    "\n",
    "embeds = pd.DataFrame(embeds).T.astype(float)\n",
    "embeds"
   ],
   "id": "7bd5b9f17562db4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2891 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e63c43b7ff149ebacc1ef5f59385562"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "                  0         1         2         3         4         5     \\\n",
       "baddie        0.617632  3.677328  2.646231  1.066725  1.290422 -0.615204   \n",
       "unblushingly -0.780811  4.523852  3.454993  0.153665  0.672691 -3.011597   \n",
       "insensibly   -0.447130  4.485464  1.775921  0.256286  1.148121 -1.091852   \n",
       "parenthesize -1.802119  3.565008  4.939508  0.087262  1.035947 -2.990864   \n",
       "insatiate    -0.485329  3.673390  2.670759  0.127345  0.541745 -2.871100   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "popularly     1.089802  4.918963  2.048598 -0.072903  1.971993 -2.231852   \n",
       "unable        0.111263  6.062789  3.109928  1.477149  0.907181 -3.777168   \n",
       "wench         1.319315  3.614148  3.027516 -0.368783  0.113268 -1.586929   \n",
       "fanaticize   -0.334161  4.565280  2.965562  0.823084  0.807213 -0.316923   \n",
       "charting     -0.469134  4.329114  4.189485 -0.114202 -0.264168 -3.403979   \n",
       "\n",
       "                  6         7         8         9     ...      2038      2039  \\\n",
       "baddie        0.670158  1.840440 -1.621486  1.008968  ... -0.722906  2.831227   \n",
       "unblushingly  1.730513  2.812676 -0.904837  2.997128  ... -1.004646  3.110042   \n",
       "insensibly    0.998185  3.691133  0.363665  1.720003  ... -3.294988  5.081615   \n",
       "parenthesize  0.296555  2.672847 -1.835698  3.704694  ... -1.855984  1.303200   \n",
       "insatiate     1.666120  2.899798  0.152642  1.901984  ... -1.974949  2.901847   \n",
       "...                ...       ...       ...       ...  ...       ...       ...   \n",
       "popularly     2.272493  2.100999  0.033533  3.741592  ... -2.732045  3.377771   \n",
       "unable        3.525682  2.249653  0.286584  2.333573  ... -1.833538  5.179575   \n",
       "wench         2.954784  3.827778 -0.045522  1.795469  ... -0.864732  2.597534   \n",
       "fanaticize    0.539709  2.069281 -0.749703  3.892582  ... -2.658002  2.438370   \n",
       "charting      0.830905  3.381310 -1.585757  4.038630  ... -1.787617  2.782739   \n",
       "\n",
       "                  2040      2041      2042      2043      2044      2045  \\\n",
       "baddie       -2.334162 -0.939548  1.865410 -2.632090  4.180321 -2.140123   \n",
       "unblushingly -2.377111 -1.208966  0.706215 -3.520344  1.515211 -3.186903   \n",
       "insensibly   -1.825445  0.847631 -0.042128 -2.111433  0.839492 -3.386798   \n",
       "parenthesize -1.128216 -1.122571  2.355582 -0.972989 -0.019990 -2.025034   \n",
       "insatiate    -1.563233  0.150040 -0.821032 -1.307971  2.909365 -1.170126   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "popularly    -3.337066  0.894094  1.947781 -2.443411  0.744422 -2.858145   \n",
       "unable       -3.770607 -0.577586 -0.271112 -1.049749  1.283552 -2.703760   \n",
       "wench        -1.394352 -2.037887  0.309999 -3.030828  4.684218 -3.293120   \n",
       "fanaticize   -1.762910  0.149773 -0.147724 -1.470575  1.400764 -2.487698   \n",
       "charting     -3.159532 -1.442757  1.991861 -2.474644 -0.200977 -2.313854   \n",
       "\n",
       "                  2046      2047  \n",
       "baddie       -1.898184 -1.185073  \n",
       "unblushingly -2.715932 -2.195801  \n",
       "insensibly   -4.152276  0.360679  \n",
       "parenthesize -1.709530 -1.170313  \n",
       "insatiate    -4.494101 -0.393104  \n",
       "...                ...       ...  \n",
       "popularly    -3.858637 -0.828052  \n",
       "unable       -4.763577  0.064372  \n",
       "wench        -3.587358 -1.720351  \n",
       "fanaticize   -2.895077 -1.743315  \n",
       "charting     -3.956381 -1.231180  \n",
       "\n",
       "[46246 rows x 2048 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baddie</th>\n",
       "      <td>0.617632</td>\n",
       "      <td>3.677328</td>\n",
       "      <td>2.646231</td>\n",
       "      <td>1.066725</td>\n",
       "      <td>1.290422</td>\n",
       "      <td>-0.615204</td>\n",
       "      <td>0.670158</td>\n",
       "      <td>1.840440</td>\n",
       "      <td>-1.621486</td>\n",
       "      <td>1.008968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.722906</td>\n",
       "      <td>2.831227</td>\n",
       "      <td>-2.334162</td>\n",
       "      <td>-0.939548</td>\n",
       "      <td>1.865410</td>\n",
       "      <td>-2.632090</td>\n",
       "      <td>4.180321</td>\n",
       "      <td>-2.140123</td>\n",
       "      <td>-1.898184</td>\n",
       "      <td>-1.185073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unblushingly</th>\n",
       "      <td>-0.780811</td>\n",
       "      <td>4.523852</td>\n",
       "      <td>3.454993</td>\n",
       "      <td>0.153665</td>\n",
       "      <td>0.672691</td>\n",
       "      <td>-3.011597</td>\n",
       "      <td>1.730513</td>\n",
       "      <td>2.812676</td>\n",
       "      <td>-0.904837</td>\n",
       "      <td>2.997128</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.004646</td>\n",
       "      <td>3.110042</td>\n",
       "      <td>-2.377111</td>\n",
       "      <td>-1.208966</td>\n",
       "      <td>0.706215</td>\n",
       "      <td>-3.520344</td>\n",
       "      <td>1.515211</td>\n",
       "      <td>-3.186903</td>\n",
       "      <td>-2.715932</td>\n",
       "      <td>-2.195801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insensibly</th>\n",
       "      <td>-0.447130</td>\n",
       "      <td>4.485464</td>\n",
       "      <td>1.775921</td>\n",
       "      <td>0.256286</td>\n",
       "      <td>1.148121</td>\n",
       "      <td>-1.091852</td>\n",
       "      <td>0.998185</td>\n",
       "      <td>3.691133</td>\n",
       "      <td>0.363665</td>\n",
       "      <td>1.720003</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.294988</td>\n",
       "      <td>5.081615</td>\n",
       "      <td>-1.825445</td>\n",
       "      <td>0.847631</td>\n",
       "      <td>-0.042128</td>\n",
       "      <td>-2.111433</td>\n",
       "      <td>0.839492</td>\n",
       "      <td>-3.386798</td>\n",
       "      <td>-4.152276</td>\n",
       "      <td>0.360679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parenthesize</th>\n",
       "      <td>-1.802119</td>\n",
       "      <td>3.565008</td>\n",
       "      <td>4.939508</td>\n",
       "      <td>0.087262</td>\n",
       "      <td>1.035947</td>\n",
       "      <td>-2.990864</td>\n",
       "      <td>0.296555</td>\n",
       "      <td>2.672847</td>\n",
       "      <td>-1.835698</td>\n",
       "      <td>3.704694</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.855984</td>\n",
       "      <td>1.303200</td>\n",
       "      <td>-1.128216</td>\n",
       "      <td>-1.122571</td>\n",
       "      <td>2.355582</td>\n",
       "      <td>-0.972989</td>\n",
       "      <td>-0.019990</td>\n",
       "      <td>-2.025034</td>\n",
       "      <td>-1.709530</td>\n",
       "      <td>-1.170313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insatiate</th>\n",
       "      <td>-0.485329</td>\n",
       "      <td>3.673390</td>\n",
       "      <td>2.670759</td>\n",
       "      <td>0.127345</td>\n",
       "      <td>0.541745</td>\n",
       "      <td>-2.871100</td>\n",
       "      <td>1.666120</td>\n",
       "      <td>2.899798</td>\n",
       "      <td>0.152642</td>\n",
       "      <td>1.901984</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.974949</td>\n",
       "      <td>2.901847</td>\n",
       "      <td>-1.563233</td>\n",
       "      <td>0.150040</td>\n",
       "      <td>-0.821032</td>\n",
       "      <td>-1.307971</td>\n",
       "      <td>2.909365</td>\n",
       "      <td>-1.170126</td>\n",
       "      <td>-4.494101</td>\n",
       "      <td>-0.393104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popularly</th>\n",
       "      <td>1.089802</td>\n",
       "      <td>4.918963</td>\n",
       "      <td>2.048598</td>\n",
       "      <td>-0.072903</td>\n",
       "      <td>1.971993</td>\n",
       "      <td>-2.231852</td>\n",
       "      <td>2.272493</td>\n",
       "      <td>2.100999</td>\n",
       "      <td>0.033533</td>\n",
       "      <td>3.741592</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.732045</td>\n",
       "      <td>3.377771</td>\n",
       "      <td>-3.337066</td>\n",
       "      <td>0.894094</td>\n",
       "      <td>1.947781</td>\n",
       "      <td>-2.443411</td>\n",
       "      <td>0.744422</td>\n",
       "      <td>-2.858145</td>\n",
       "      <td>-3.858637</td>\n",
       "      <td>-0.828052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unable</th>\n",
       "      <td>0.111263</td>\n",
       "      <td>6.062789</td>\n",
       "      <td>3.109928</td>\n",
       "      <td>1.477149</td>\n",
       "      <td>0.907181</td>\n",
       "      <td>-3.777168</td>\n",
       "      <td>3.525682</td>\n",
       "      <td>2.249653</td>\n",
       "      <td>0.286584</td>\n",
       "      <td>2.333573</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.833538</td>\n",
       "      <td>5.179575</td>\n",
       "      <td>-3.770607</td>\n",
       "      <td>-0.577586</td>\n",
       "      <td>-0.271112</td>\n",
       "      <td>-1.049749</td>\n",
       "      <td>1.283552</td>\n",
       "      <td>-2.703760</td>\n",
       "      <td>-4.763577</td>\n",
       "      <td>0.064372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wench</th>\n",
       "      <td>1.319315</td>\n",
       "      <td>3.614148</td>\n",
       "      <td>3.027516</td>\n",
       "      <td>-0.368783</td>\n",
       "      <td>0.113268</td>\n",
       "      <td>-1.586929</td>\n",
       "      <td>2.954784</td>\n",
       "      <td>3.827778</td>\n",
       "      <td>-0.045522</td>\n",
       "      <td>1.795469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.864732</td>\n",
       "      <td>2.597534</td>\n",
       "      <td>-1.394352</td>\n",
       "      <td>-2.037887</td>\n",
       "      <td>0.309999</td>\n",
       "      <td>-3.030828</td>\n",
       "      <td>4.684218</td>\n",
       "      <td>-3.293120</td>\n",
       "      <td>-3.587358</td>\n",
       "      <td>-1.720351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fanaticize</th>\n",
       "      <td>-0.334161</td>\n",
       "      <td>4.565280</td>\n",
       "      <td>2.965562</td>\n",
       "      <td>0.823084</td>\n",
       "      <td>0.807213</td>\n",
       "      <td>-0.316923</td>\n",
       "      <td>0.539709</td>\n",
       "      <td>2.069281</td>\n",
       "      <td>-0.749703</td>\n",
       "      <td>3.892582</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.658002</td>\n",
       "      <td>2.438370</td>\n",
       "      <td>-1.762910</td>\n",
       "      <td>0.149773</td>\n",
       "      <td>-0.147724</td>\n",
       "      <td>-1.470575</td>\n",
       "      <td>1.400764</td>\n",
       "      <td>-2.487698</td>\n",
       "      <td>-2.895077</td>\n",
       "      <td>-1.743315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charting</th>\n",
       "      <td>-0.469134</td>\n",
       "      <td>4.329114</td>\n",
       "      <td>4.189485</td>\n",
       "      <td>-0.114202</td>\n",
       "      <td>-0.264168</td>\n",
       "      <td>-3.403979</td>\n",
       "      <td>0.830905</td>\n",
       "      <td>3.381310</td>\n",
       "      <td>-1.585757</td>\n",
       "      <td>4.038630</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.787617</td>\n",
       "      <td>2.782739</td>\n",
       "      <td>-3.159532</td>\n",
       "      <td>-1.442757</td>\n",
       "      <td>1.991861</td>\n",
       "      <td>-2.474644</td>\n",
       "      <td>-0.200977</td>\n",
       "      <td>-2.313854</td>\n",
       "      <td>-3.956381</td>\n",
       "      <td>-1.231180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46246 rows × 2048 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T14:01:10.964173Z",
     "start_time": "2025-07-10T14:01:04.838215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ft_baseline = pd.read_csv('../../data/embeds/fastText_CommonCrawl.csv', index_col=0)\n",
    "\n",
    "# Comparing\n",
    "to_compare = {\n",
    "    'ft_baseline': ft_baseline,\n",
    "    'Llama_3.2_1B': embeds\n",
    "}\n",
    "\n",
    "# Aligning for fair comparison\n",
    "to_compare = dict(zip(to_compare.keys(), multi_inner_align(to_compare.values())))\n",
    "\n",
    "# Standardizing\n",
    "to_compare = {name: standardize(embed) for name, embed in to_compare.items()}\n",
    "\n",
    "# Loading norm data\n",
    "norms = pd.read_csv('../../data/psychNorms/psychNorms_processed.zip', index_col=0, low_memory=False, compression='zip')\n",
    "norms_meta = pd.read_csv('../../data/psychNorms/psychNorms_metadata_processed.csv', index_col='norm')\n",
    "norms"
   ],
   "id": "d64ecd6f9ea3a4cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             frequency_lund  frequency_kucera  frequency_subtlexus  \\\n",
       "word                                                                 \n",
       "'em                     0.0               NaN                  NaN   \n",
       "'neath                  0.0               NaN                  NaN   \n",
       "'re                     0.0               NaN                  NaN   \n",
       "'shun                   0.0               NaN                  NaN   \n",
       "'tis                    0.0               NaN                  NaN   \n",
       "...                     ...               ...                  ...   \n",
       "shrick                  NaN               NaN                  NaN   \n",
       "post office             NaN               NaN                  NaN   \n",
       "fishing rod             NaN               NaN                  NaN   \n",
       "March                   NaN               NaN                  NaN   \n",
       "May                     NaN               NaN                  NaN   \n",
       "\n",
       "             frequency_subtlexuk  frequency_blog_gimenes  \\\n",
       "word                                                       \n",
       "'em                          NaN                     NaN   \n",
       "'neath                       NaN                     NaN   \n",
       "'re                          NaN                     NaN   \n",
       "'shun                        NaN                     NaN   \n",
       "'tis                         NaN                     NaN   \n",
       "...                          ...                     ...   \n",
       "shrick                       NaN                     NaN   \n",
       "post office                  NaN                     NaN   \n",
       "fishing rod                  NaN                     NaN   \n",
       "March                        NaN                     NaN   \n",
       "May                          NaN                     NaN   \n",
       "\n",
       "             frequency_twitter_gimenes  frequency_news_gimenes  \\\n",
       "word                                                             \n",
       "'em                                NaN                     NaN   \n",
       "'neath                             NaN                     NaN   \n",
       "'re                                NaN                     NaN   \n",
       "'shun                              NaN                     NaN   \n",
       "'tis                               NaN                     NaN   \n",
       "...                                ...                     ...   \n",
       "shrick                             NaN                     NaN   \n",
       "post office                        NaN                     NaN   \n",
       "fishing rod                        NaN                     NaN   \n",
       "March                              NaN                     NaN   \n",
       "May                                NaN                     NaN   \n",
       "\n",
       "             frequency_written_cobuild  frequency_spoken_cobuild  \\\n",
       "word                                                               \n",
       "'em                             1.3617                    1.9138   \n",
       "'neath                          0.0000                    0.0000   \n",
       "'re                             0.9031                    1.6335   \n",
       "'shun                           0.0000                    0.0000   \n",
       "'tis                            0.4771                    0.6021   \n",
       "...                                ...                       ...   \n",
       "shrick                             NaN                       NaN   \n",
       "post office                        NaN                       NaN   \n",
       "fishing rod                        NaN                       NaN   \n",
       "March                              NaN                       NaN   \n",
       "May                                NaN                       NaN   \n",
       "\n",
       "             context_diversity_subtlexus  ...  person_vanarsdall  \\\n",
       "word                                      ...                      \n",
       "'em                                  NaN  ...                NaN   \n",
       "'neath                               NaN  ...                NaN   \n",
       "'re                                  NaN  ...                NaN   \n",
       "'shun                                NaN  ...                NaN   \n",
       "'tis                                 NaN  ...                NaN   \n",
       "...                                  ...  ...                ...   \n",
       "shrick                               NaN  ...                NaN   \n",
       "post office                          NaN  ...                NaN   \n",
       "fishing rod                          NaN  ...                NaN   \n",
       "March                                NaN  ...                NaN   \n",
       "May                                  NaN  ...                NaN   \n",
       "\n",
       "             goals_vanarsdall  movement_vanarsdall  concreteness_vanarsdall  \\\n",
       "word                                                                          \n",
       "'em                       NaN                  NaN                      NaN   \n",
       "'neath                    NaN                  NaN                      NaN   \n",
       "'re                       NaN                  NaN                      NaN   \n",
       "'shun                     NaN                  NaN                      NaN   \n",
       "'tis                      NaN                  NaN                      NaN   \n",
       "...                       ...                  ...                      ...   \n",
       "shrick                    NaN                  NaN                      NaN   \n",
       "post office               NaN                  NaN                      NaN   \n",
       "fishing rod               NaN                  NaN                      NaN   \n",
       "March                     NaN                  NaN                      NaN   \n",
       "May                       NaN                  NaN                      NaN   \n",
       "\n",
       "             familiarity_vanarsdall  imageability_vanarsdall  \\\n",
       "word                                                           \n",
       "'em                             NaN                      NaN   \n",
       "'neath                          NaN                      NaN   \n",
       "'re                             NaN                      NaN   \n",
       "'shun                           NaN                      NaN   \n",
       "'tis                            NaN                      NaN   \n",
       "...                             ...                      ...   \n",
       "shrick                          NaN                      NaN   \n",
       "post office                     NaN                      NaN   \n",
       "fishing rod                     NaN                      NaN   \n",
       "March                           NaN                      NaN   \n",
       "May                             NaN                      NaN   \n",
       "\n",
       "             familiarity_fear  aoa_fear  imageability_fear  \\\n",
       "word                                                         \n",
       "'em                       NaN       NaN                NaN   \n",
       "'neath                    NaN       NaN                NaN   \n",
       "'re                       NaN       NaN                NaN   \n",
       "'shun                     NaN       NaN                NaN   \n",
       "'tis                      NaN       NaN                NaN   \n",
       "...                       ...       ...                ...   \n",
       "shrick                   2.62      4.38               2.93   \n",
       "post office              3.79      3.07               5.29   \n",
       "fishing rod              2.29      3.38               5.64   \n",
       "March                    3.43      2.76               3.50   \n",
       "May                      4.10      2.86               2.79   \n",
       "\n",
       "             sensory_experience_juhasz2013  \n",
       "word                                        \n",
       "'em                                    NaN  \n",
       "'neath                                 NaN  \n",
       "'re                                    NaN  \n",
       "'shun                                  NaN  \n",
       "'tis                                   NaN  \n",
       "...                                    ...  \n",
       "shrick                                 NaN  \n",
       "post office                            NaN  \n",
       "fishing rod                            NaN  \n",
       "March                                  NaN  \n",
       "May                                    NaN  \n",
       "\n",
       "[107085 rows x 291 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency_lund</th>\n",
       "      <th>frequency_kucera</th>\n",
       "      <th>frequency_subtlexus</th>\n",
       "      <th>frequency_subtlexuk</th>\n",
       "      <th>frequency_blog_gimenes</th>\n",
       "      <th>frequency_twitter_gimenes</th>\n",
       "      <th>frequency_news_gimenes</th>\n",
       "      <th>frequency_written_cobuild</th>\n",
       "      <th>frequency_spoken_cobuild</th>\n",
       "      <th>context_diversity_subtlexus</th>\n",
       "      <th>...</th>\n",
       "      <th>person_vanarsdall</th>\n",
       "      <th>goals_vanarsdall</th>\n",
       "      <th>movement_vanarsdall</th>\n",
       "      <th>concreteness_vanarsdall</th>\n",
       "      <th>familiarity_vanarsdall</th>\n",
       "      <th>imageability_vanarsdall</th>\n",
       "      <th>familiarity_fear</th>\n",
       "      <th>aoa_fear</th>\n",
       "      <th>imageability_fear</th>\n",
       "      <th>sensory_experience_juhasz2013</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'em</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3617</td>\n",
       "      <td>1.9138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'neath</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'re</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9031</td>\n",
       "      <td>1.6335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'shun</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'tis</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4771</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shrick</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.62</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2.93</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post office</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.07</td>\n",
       "      <td>5.29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fishing rod</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.29</td>\n",
       "      <td>3.38</td>\n",
       "      <td>5.64</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>March</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2.79</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107085 rows × 291 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-10T14:01:21.820963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = run_rca(to_compare, norms, norms_meta, n_jobs=10)\n",
    "results"
   ],
   "id": "d9e814895af96c69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5616ba80b9394e74a2d0c858fa628ebb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ft_baseline:   0%|          | 0/291 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab7f0fad6a39424f9df858065d1102e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adding norm category\n",
    "results['norm_category'] = (\n",
    "    results['norm']\n",
    "    .apply(lambda norm: norms_meta.loc[norm]['category'])\n",
    "    .replace({'_': ' '}, regex=True)\n",
    ")\n",
    "\n",
    "results_avg = (\n",
    "    results[['norm_category', 'embed', 'r2_mean']]\n",
    "    .groupby(['norm_category', 'embed'], as_index=False).median()\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "results_avg_piv = results_avg.pivot(columns='embed', index='norm_category', values='r2_mean')\n",
    "results_avg_piv.round(2)"
   ],
   "id": "ca850a3565a89667",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Finding the top-performing fmri_text_denoise\n",
    "sorted_overall = results_avg_piv.mean().sort_values(ascending=False)\n",
    "sorted_overall"
   ],
   "id": "bb67cb3908d8b8ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving",
   "id": "7fc0caf9b78bf753"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_performer = None\n",
    "top_performer.to_csv('../../data/embeds/Llama_X_XB.csv')\n",
    "\n"
   ],
   "id": "a1497897bb5869f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
