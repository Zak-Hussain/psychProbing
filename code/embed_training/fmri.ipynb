{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-08T12:11:03.871378Z",
     "start_time": "2025-07-08T12:10:59.649449Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from embeds import fix_corrupt, multi_inner_align, standardize\n",
    "import numpy as np\n",
    "from hypertools.tools import align\n",
    "import torch\n",
    "\n",
    "sys.path.append('..')\n",
    "from rca.rca import run_rca"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhussain/miniconda3/envs/psychProbing_env/lib/python3.10/site-packages/hypertools/config.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cognival",
   "id": "d633366108d33797"
  },
  {
   "cell_type": "code",
   "source": [
    "def read_txt(f) -> pd.DataFrame:\n",
    "    \"\"\"For reading the cognival data\"\"\"\n",
    "    pulled = {}\n",
    "    for line in f:\n",
    "        word, *vec = line.split()\n",
    "        pulled[word] = vec\n",
    "    pulled = fix_corrupt(pulled)\n",
    "    return pd.DataFrame(pulled).T.astype(float)\n",
    "\n",
    "def read_individual_fmri(path: str) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "\n",
    "    for f_name in os.listdir(path):\n",
    "\n",
    "        if f_name.endswith('.txt'):\n",
    "            with open(path + f_name, 'r') as f:\n",
    "                dfs.append(read_txt(f))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # Align indices\n",
    "    dfs = list(multi_inner_align(dfs))\n",
    "\n",
    "    return dfs\n",
    "\n",
    "fmris_text_cognival = read_individual_fmri('../../data/fmri_text_cognival/')\n",
    "fmris_speech_cognival = read_individual_fmri('../../data/fmri_speech_cognival/')\n",
    "\n",
    "len(fmris_text_cognival), len(fmris_speech_cognival)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-08T12:11:08.306810Z",
     "start_time": "2025-07-08T12:11:05.453797Z"
    }
   },
   "id": "fb6a39731d1dc10e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 27 8 8 8\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:20:13.059026Z",
     "start_time": "2025-06-10T14:20:10.358751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standardize before hyper-aligning\n",
    "fmris_text_cognival = [standardize(df) for df in fmris_text_cognival]\n",
    "fmris_speech_cognival = [standardize(df) for df in fmris_speech_cognival]\n",
    "\n",
    "# --- Hyper aligning individuals ---\n",
    "def hyper_align(dfs: list) -> pd.DataFrame:\n",
    "    df = np.mean(align(dfs, align='hyper'), axis=0)\n",
    "    return pd.DataFrame(df, index=dfs[0].index)\n",
    "\n",
    "fMRI_text_cognival = hyper_align(fmris_text_cognival)\n",
    "fMRI_speech_cognival = hyper_align(fmris_speech_cognival)"
   ],
   "id": "eaf914aa6621a248",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Denoised (Antonia)",
   "id": "eb927c09ea5f2abf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def read_from_torch(f_path) -> pd.DataFrame:\n",
    "    fmri_dict = torch.load(f_path, weights_only=False)\n",
    "    voc, vecs = fmri_dict['dico'], fmri_dict['vectors'].numpy()\n",
    "    return pd.DataFrame(vecs, index=voc, dtype=float)\n",
    "\n",
    "fMRI_text_denoise_128d = read_from_torch('')\n",
    "fMRI_text_denoise_256d = read_from_torch('')\n",
    "fMRI_text_denoise_512d = read_from_torch('')\n",
    "\n",
    "fMRI_text_denoise_128d"
   ],
   "id": "c419b6277f725e2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T12:18:59.643271Z",
     "start_time": "2025-07-09T12:18:59.532275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Finding best denoise dimensionality\n",
    "to_compare = {\n",
    "    'fMRI_text_cognival': fMRI_text_cognival.copy(),\n",
    "    'fMRI_text_denoise_128d': fMRI_text_denoise_128d.copy(),\n",
    "    'fMRI_text_denoise_256d': fMRI_text_denoise_256d.copy(),\n",
    "    'fMRI_text_denoise_512d': fMRI_text_denoise_512d.copy()\n",
    "}\n",
    "\n",
    "# Aligning for fair comparison\n",
    "to_compare = dict(zip(to_compare.keys(), multi_inner_align(to_compare.values())))\n",
    "\n",
    "# Standardizing\n",
    "to_compare = {name: standardize(embed) for name, embed in to_compare.items()}\n",
    "\n",
    "# Loading norm data\n",
    "norms = pd.read_csv('../../data/psychNorms/psychNorms_processed.zip', index_col=0, low_memory=False, compression='zip')\n",
    "norms_meta = pd.read_csv('../../data/psychNorms/psychNorms_metadata_processed.csv', index_col='norm')\n",
    "norms"
   ],
   "id": "bcd19338dd7bdf67",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fMRI_text_denoise_128d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m to_compare \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfMRI_text_denoise_128d\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mfMRI_text_denoise_128d\u001B[49m,\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfMRI_text_denoise_256d\u001B[39m\u001B[38;5;124m'\u001B[39m: fMRI_text_denoise_256d,\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfMRI_text_denoise_512d\u001B[39m\u001B[38;5;124m'\u001B[39m: fMRI_text_denoise_512d\n\u001B[1;32m      5\u001B[0m }\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Aligning for controlled comparison\u001B[39;00m\n\u001B[1;32m      8\u001B[0m to_compare \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(to_compare\u001B[38;5;241m.\u001B[39mkeys(), multi_inner_align(to_compare\u001B[38;5;241m.\u001B[39mvalues())))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'fMRI_text_denoise_128d' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:22:26.794725Z",
     "start_time": "2025-06-10T14:20:23.673024Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21b4d31214de4e16b98dcf0152163134"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fMRI_text_cognival_participant:   0%|          | 0/88 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff752e2b98854d0290f6072a0ab40afa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  norm  train_n   r2_mean     r2_sd check\n",
      "56         n_semantic_neighbors_shaoul      881  0.018786  0.037550  pass\n",
      "29                    haptic_lancaster      712  0.007802  0.014477  pass\n",
      "57  distance_semantic_neighbors_shaoul      881  0.005233  0.035730  pass\n",
      "45                    valence_mohammad      501  0.003122  0.016986  pass\n",
      "36                  hand_arm_lancaster      712 -0.001156  0.024811  pass\n",
      "50                    arousal_mohammad      501 -0.002012  0.016566  pass\n",
      "53                  dominance_mohammad      501 -0.004229  0.026664  pass\n",
      "32             interoceptive_lancaster      712 -0.004505  0.010642  pass\n",
      "16                prevalence_brysbaert      718 -0.005420  0.009512  pass\n",
      "74              naming_accuracy_balota      882 -0.005446  0.009649  pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fMRI_text_cognival:   0%|          | 0/88 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4212b05393f746dc9c9cb058cb36e861"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 norm  train_n   r2_mean     r2_sd check\n",
      "2                 frequency_subtlexus      877  0.088550  0.109966  pass\n",
      "3                 frequency_subtlexuk      880  0.081935  0.100987  pass\n",
      "0                      frequency_lund      905  0.079998  0.099975  pass\n",
      "13     context_diversity_news_gimenes      884  0.078482  0.104070  pass\n",
      "6              frequency_news_gimenes      884  0.078479  0.105064  pass\n",
      "12  context_diversity_twitter_gimenes      884  0.076833  0.102312  pass\n",
      "1                    frequency_kucera      872  0.076807  0.136276  pass\n",
      "5           frequency_twitter_gimenes      884  0.076595  0.102471  pass\n",
      "11     context_diversity_blog_gimenes      884  0.076045  0.109708  pass\n",
      "4              frequency_blog_gimenes      884  0.075911  0.112028  pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fMRI_text_denoise_128d_participant:   0%|          | 0/88 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c650e3acc2094df481fda1fd1fee9f17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                norm  train_n   r2_mean     r2_sd check\n",
      "29                  haptic_lancaster      712  0.027954  0.018120  pass\n",
      "68  visual_lexical_accuracy_keuleers      793 -0.001844  0.002291  pass\n",
      "32           interoceptive_lancaster      712 -0.002735  0.005123  pass\n",
      "82                 perc_known_winter      599 -0.003276  0.004154  pass\n",
      "66    visual_lexical_accuracy_balota      882 -0.003288  0.003635  pass\n",
      "50                  arousal_mohammad      501 -0.003595  0.008310  pass\n",
      "28                auditory_lancaster      712 -0.003620  0.011794  pass\n",
      "18                     aoa_brysbaert      519 -0.003833  0.004064  pass\n",
      "16              prevalence_brysbaert      718 -0.004339  0.004053  pass\n",
      "36                hand_arm_lancaster      712 -0.004518  0.027546  pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fMRI_text_denoise_256d_participant:   0%|          | 0/88 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de49e0b7aa8c43a9b22265a8121c90d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                norm  train_n   r2_mean     r2_sd check\n",
      "28                auditory_lancaster      712  0.008462  0.004362  pass\n",
      "32           interoceptive_lancaster      712 -0.000387  0.005901  pass\n",
      "68  visual_lexical_accuracy_keuleers      793 -0.002260  0.002215  pass\n",
      "66    visual_lexical_accuracy_balota      882 -0.003531  0.003644  pass\n",
      "50                  arousal_mohammad      501 -0.003619  0.008356  pass\n",
      "18                     aoa_brysbaert      519 -0.003827  0.004000  pass\n",
      "16              prevalence_brysbaert      718 -0.004493  0.004113  pass\n",
      "67   visual_lexical_accuracy_mandera      718 -0.004872  0.007874  pass\n",
      "82                 perc_known_winter      599 -0.005466  0.003974  pass\n",
      "61            cue_probability_nelson      431 -0.005599  0.008637  pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fMRI_text_denoise_512d_participant:   0%|          | 0/88 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28ee363093f740189261a94637621edb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   norm  train_n   r2_mean     r2_sd check\n",
      "28                   auditory_lancaster      712  0.036367  0.023751  pass\n",
      "77  recognition_memory_accuracy_cortese      292  0.024766  0.037466  pass\n",
      "35               mouth_throat_lancaster      712  0.007442  0.007997  pass\n",
      "56          n_semantic_neighbors_shaoul      881  0.004639  0.009409  pass\n",
      "82                    perc_known_winter      599  0.003538  0.007849  pass\n",
      "57   distance_semantic_neighbors_shaoul      881  0.000957  0.007721  pass\n",
      "29                     haptic_lancaster      712  0.000274  0.010233  pass\n",
      "36                   hand_arm_lancaster      712 -0.000448  0.016242  pass\n",
      "32              interoceptive_lancaster      712 -0.001242  0.006653  pass\n",
      "68     visual_lexical_accuracy_keuleers      793 -0.001707  0.002498  pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fMRI_text_denoise_128d:   0%|          | 0/88 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17c6572fd2fd43ee9161a465ae208b35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   norm  train_n   r2_mean     r2_sd check\n",
      "28                   auditory_lancaster      712  0.032517  0.015234  pass\n",
      "29                     haptic_lancaster      712  0.011990  0.016320  pass\n",
      "77  recognition_memory_accuracy_cortese      292  0.011964  0.016190  pass\n",
      "36                   hand_arm_lancaster      712  0.005515  0.039182  pass\n",
      "56          n_semantic_neighbors_shaoul      881 -0.002658  0.012463  pass\n",
      "22                 concreteness_glasgow      303 -0.003258  0.023928  pass\n",
      "32              interoceptive_lancaster      712 -0.003628  0.017648  pass\n",
      "82                    perc_known_winter      599 -0.003841  0.003803  pass\n",
      "18                        aoa_brysbaert      519 -0.003900  0.004064  pass\n",
      "16                 prevalence_brysbaert      718 -0.004332  0.004230  pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fMRI_text_denoise_256d:   0%|          | 0/88 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "859202e91ccd46698435dcef5c394184"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   norm  train_n   r2_mean     r2_sd check\n",
      "28                   auditory_lancaster      712  0.060283  0.025268  pass\n",
      "23                 imageability_glasgow      303  0.027633  0.072153  pass\n",
      "77  recognition_memory_accuracy_cortese      292  0.022207  0.042576  pass\n",
      "21               concreteness_brysbaert      712  0.001204  0.016222  pass\n",
      "32              interoceptive_lancaster      712  0.000119  0.015421  pass\n",
      "82                    perc_known_winter      599 -0.000346  0.002209  pass\n",
      "50                     arousal_mohammad      501 -0.000626  0.009732  pass\n",
      "29                     haptic_lancaster      712 -0.001221  0.010973  pass\n",
      "57   distance_semantic_neighbors_shaoul      881 -0.001361  0.009737  pass\n",
      "68     visual_lexical_accuracy_keuleers      793 -0.002046  0.002284  pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fMRI_text_denoise_512d:   0%|          | 0/88 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc05d4dd5d3a4148b13a57c212573ce6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  norm  train_n   r2_mean     r2_sd check\n",
      "28                  auditory_lancaster      712  0.047891  0.009600  pass\n",
      "23                imageability_glasgow      303  0.034378  0.064394  pass\n",
      "22                concreteness_glasgow      303  0.031313  0.051942  pass\n",
      "56         n_semantic_neighbors_shaoul      881  0.005782  0.009343  pass\n",
      "32             interoceptive_lancaster      712  0.005345  0.003233  pass\n",
      "57  distance_semantic_neighbors_shaoul      881  0.003011  0.008346  pass\n",
      "35              mouth_throat_lancaster      712  0.001949  0.008341  pass\n",
      "29                    haptic_lancaster      712  0.001466  0.013712  pass\n",
      "36                  hand_arm_lancaster      712 -0.000632  0.015734  pass\n",
      "68    visual_lexical_accuracy_keuleers      793 -0.001959  0.002592  pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                              embed embed_type                           norm  \\\n",
       "0    fMRI_text_cognival_participant       None                 frequency_lund   \n",
       "1    fMRI_text_cognival_participant       None               frequency_kucera   \n",
       "2    fMRI_text_cognival_participant       None            frequency_subtlexus   \n",
       "3    fMRI_text_cognival_participant       None            frequency_subtlexuk   \n",
       "4    fMRI_text_cognival_participant       None         frequency_blog_gimenes   \n",
       "..                              ...        ...                            ...   \n",
       "699          fMRI_text_denoise_512d       None           iconicity_winter2023   \n",
       "700          fMRI_text_denoise_512d       None                     aoa_schock   \n",
       "701          fMRI_text_denoise_512d       None                  aoa_rt_schock   \n",
       "702          fMRI_text_denoise_512d       None           iconicity_winter2017   \n",
       "703          fMRI_text_denoise_512d       None  sensory_experience_juhasz2013   \n",
       "\n",
       "     train_n  test_n     p   r2_mean     r2_sd check  \n",
       "0        905     227  1000 -0.028576  0.029151  pass  \n",
       "1        872     219  1000 -0.049635  0.057808  pass  \n",
       "2        877     220  1000 -0.046786  0.077060  pass  \n",
       "3        880     221  1000 -0.037376  0.066358  pass  \n",
       "4        884     222  1000 -0.026952  0.036523  pass  \n",
       "..       ...     ...   ...       ...       ...   ...  \n",
       "699      599     150   512 -0.035473  0.027085  pass  \n",
       "700      126      32   512 -0.027804  0.034623  pass  \n",
       "701      126      32   512 -0.041706  0.036968  pass  \n",
       "702      320      80   512 -0.068191  0.064661  pass  \n",
       "703      445     112   512 -0.025952  0.013836  pass  \n",
       "\n",
       "[704 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed</th>\n",
       "      <th>embed_type</th>\n",
       "      <th>norm</th>\n",
       "      <th>train_n</th>\n",
       "      <th>test_n</th>\n",
       "      <th>p</th>\n",
       "      <th>r2_mean</th>\n",
       "      <th>r2_sd</th>\n",
       "      <th>check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fMRI_text_cognival_participant</td>\n",
       "      <td>None</td>\n",
       "      <td>frequency_lund</td>\n",
       "      <td>905</td>\n",
       "      <td>227</td>\n",
       "      <td>1000</td>\n",
       "      <td>-0.028576</td>\n",
       "      <td>0.029151</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fMRI_text_cognival_participant</td>\n",
       "      <td>None</td>\n",
       "      <td>frequency_kucera</td>\n",
       "      <td>872</td>\n",
       "      <td>219</td>\n",
       "      <td>1000</td>\n",
       "      <td>-0.049635</td>\n",
       "      <td>0.057808</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fMRI_text_cognival_participant</td>\n",
       "      <td>None</td>\n",
       "      <td>frequency_subtlexus</td>\n",
       "      <td>877</td>\n",
       "      <td>220</td>\n",
       "      <td>1000</td>\n",
       "      <td>-0.046786</td>\n",
       "      <td>0.077060</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fMRI_text_cognival_participant</td>\n",
       "      <td>None</td>\n",
       "      <td>frequency_subtlexuk</td>\n",
       "      <td>880</td>\n",
       "      <td>221</td>\n",
       "      <td>1000</td>\n",
       "      <td>-0.037376</td>\n",
       "      <td>0.066358</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fMRI_text_cognival_participant</td>\n",
       "      <td>None</td>\n",
       "      <td>frequency_blog_gimenes</td>\n",
       "      <td>884</td>\n",
       "      <td>222</td>\n",
       "      <td>1000</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>0.036523</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>fMRI_text_denoise_512d</td>\n",
       "      <td>None</td>\n",
       "      <td>iconicity_winter2023</td>\n",
       "      <td>599</td>\n",
       "      <td>150</td>\n",
       "      <td>512</td>\n",
       "      <td>-0.035473</td>\n",
       "      <td>0.027085</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>fMRI_text_denoise_512d</td>\n",
       "      <td>None</td>\n",
       "      <td>aoa_schock</td>\n",
       "      <td>126</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>-0.027804</td>\n",
       "      <td>0.034623</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>fMRI_text_denoise_512d</td>\n",
       "      <td>None</td>\n",
       "      <td>aoa_rt_schock</td>\n",
       "      <td>126</td>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>-0.041706</td>\n",
       "      <td>0.036968</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>fMRI_text_denoise_512d</td>\n",
       "      <td>None</td>\n",
       "      <td>iconicity_winter2017</td>\n",
       "      <td>320</td>\n",
       "      <td>80</td>\n",
       "      <td>512</td>\n",
       "      <td>-0.068191</td>\n",
       "      <td>0.064661</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>fMRI_text_denoise_512d</td>\n",
       "      <td>None</td>\n",
       "      <td>sensory_experience_juhasz2013</td>\n",
       "      <td>445</td>\n",
       "      <td>112</td>\n",
       "      <td>512</td>\n",
       "      <td>-0.025952</td>\n",
       "      <td>0.013836</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 9 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27,
   "source": [
    "results = run_rca(to_compare, norms, norms_meta)\n",
    "results"
   ],
   "id": "67e7ed60985f078f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plotting",
   "id": "86ccb6dea785c9c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:22:32.356912Z",
     "start_time": "2025-06-10T14:22:32.322913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adding norm category\n",
    "results['norm_category'] = (\n",
    "    results['norm']\n",
    "    .apply(lambda norm: norms_meta.loc[norm]['category'])\n",
    "    .replace({'_': ' '}, regex=True)\n",
    ")\n",
    "\n",
    "results_avg = (\n",
    "    results[['norm_category', 'embed', 'r2_mean']]\n",
    "    .groupby(['norm_category', 'embed'], as_index=False).median()\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "results_avg_piv = results_avg.pivot(columns='embed', index='norm_category', values='r2_mean')\n",
    "results_avg_piv.round(2)"
   ],
   "id": "79c5d86468192815",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embed                      fMRI_text_cognival  fMRI_text_cognival_participant  \\\n",
       "norm_category                                                                   \n",
       "age of acquisition                      -0.02                           -0.04   \n",
       "arousal                                 -0.02                           -0.03   \n",
       "auditory lexical decision               -0.03                           -0.03   \n",
       "concreteness                            -0.01                           -0.02   \n",
       "dominance                                0.00                           -0.02   \n",
       "familiarity                             -0.01                           -0.01   \n",
       "frequency                                0.08                           -0.04   \n",
       "iconicity/transparency                  -0.03                           -0.06   \n",
       "imageability                            -0.00                           -0.05   \n",
       "motor                                   -0.02                           -0.02   \n",
       "naming                                  -0.05                           -0.06   \n",
       "number of features                      -0.03                           -0.02   \n",
       "recognition memory                      -0.02                           -0.01   \n",
       "semantic decision                       -0.05                           -0.06   \n",
       "semantic diversity                       0.06                           -0.02   \n",
       "semantic neighborhood                   -0.00                           -0.02   \n",
       "sensory                                 -0.00                           -0.01   \n",
       "social/moral                            -0.02                           -0.05   \n",
       "space/time/quantity                     -0.04                           -0.05   \n",
       "valence                                 -0.04                           -0.04   \n",
       "visual lexical decision                 -0.01                           -0.01   \n",
       "\n",
       "embed                      fMRI_text_denoise_128d  \\\n",
       "norm_category                                       \n",
       "age of acquisition                          -0.03   \n",
       "arousal                                     -0.02   \n",
       "auditory lexical decision                   -0.03   \n",
       "concreteness                                -0.01   \n",
       "dominance                                   -0.02   \n",
       "familiarity                                 -0.01   \n",
       "frequency                                   -0.03   \n",
       "iconicity/transparency                      -0.05   \n",
       "imageability                                -0.02   \n",
       "motor                                       -0.02   \n",
       "naming                                      -0.06   \n",
       "number of features                          -0.02   \n",
       "recognition memory                           0.01   \n",
       "semantic decision                           -0.07   \n",
       "semantic diversity                          -0.02   \n",
       "semantic neighborhood                       -0.01   \n",
       "sensory                                     -0.01   \n",
       "social/moral                                -0.05   \n",
       "space/time/quantity                         -0.03   \n",
       "valence                                     -0.03   \n",
       "visual lexical decision                     -0.01   \n",
       "\n",
       "embed                      fMRI_text_denoise_128d_participant  \\\n",
       "norm_category                                                   \n",
       "age of acquisition                                      -0.03   \n",
       "arousal                                                 -0.03   \n",
       "auditory lexical decision                               -0.03   \n",
       "concreteness                                            -0.02   \n",
       "dominance                                               -0.01   \n",
       "familiarity                                             -0.01   \n",
       "frequency                                               -0.04   \n",
       "iconicity/transparency                                  -0.06   \n",
       "imageability                                            -0.02   \n",
       "motor                                                   -0.02   \n",
       "naming                                                  -0.06   \n",
       "number of features                                      -0.02   \n",
       "recognition memory                                      -0.01   \n",
       "semantic decision                                       -0.06   \n",
       "semantic diversity                                      -0.03   \n",
       "semantic neighborhood                                   -0.01   \n",
       "sensory                                                 -0.01   \n",
       "social/moral                                            -0.03   \n",
       "space/time/quantity                                     -0.04   \n",
       "valence                                                 -0.04   \n",
       "visual lexical decision                                 -0.01   \n",
       "\n",
       "embed                      fMRI_text_denoise_256d  \\\n",
       "norm_category                                       \n",
       "age of acquisition                          -0.03   \n",
       "arousal                                     -0.02   \n",
       "auditory lexical decision                   -0.03   \n",
       "concreteness                                -0.00   \n",
       "dominance                                   -0.03   \n",
       "familiarity                                 -0.01   \n",
       "frequency                                   -0.04   \n",
       "iconicity/transparency                      -0.05   \n",
       "imageability                                 0.01   \n",
       "motor                                       -0.01   \n",
       "naming                                      -0.06   \n",
       "number of features                          -0.02   \n",
       "recognition memory                           0.02   \n",
       "semantic decision                           -0.06   \n",
       "semantic diversity                          -0.03   \n",
       "semantic neighborhood                       -0.02   \n",
       "sensory                                     -0.01   \n",
       "social/moral                                -0.03   \n",
       "space/time/quantity                         -0.05   \n",
       "valence                                     -0.02   \n",
       "visual lexical decision                     -0.01   \n",
       "\n",
       "embed                      fMRI_text_denoise_256d_participant  \\\n",
       "norm_category                                                   \n",
       "age of acquisition                                      -0.03   \n",
       "arousal                                                 -0.02   \n",
       "auditory lexical decision                               -0.03   \n",
       "concreteness                                            -0.02   \n",
       "dominance                                               -0.02   \n",
       "familiarity                                             -0.01   \n",
       "frequency                                               -0.04   \n",
       "iconicity/transparency                                  -0.06   \n",
       "imageability                                            -0.02   \n",
       "motor                                                   -0.02   \n",
       "naming                                                  -0.06   \n",
       "number of features                                      -0.02   \n",
       "recognition memory                                      -0.01   \n",
       "semantic decision                                       -0.06   \n",
       "semantic diversity                                      -0.02   \n",
       "semantic neighborhood                                   -0.01   \n",
       "sensory                                                 -0.02   \n",
       "social/moral                                            -0.05   \n",
       "space/time/quantity                                     -0.04   \n",
       "valence                                                 -0.02   \n",
       "visual lexical decision                                 -0.01   \n",
       "\n",
       "embed                      fMRI_text_denoise_512d  \\\n",
       "norm_category                                       \n",
       "age of acquisition                          -0.03   \n",
       "arousal                                     -0.01   \n",
       "auditory lexical decision                   -0.03   \n",
       "concreteness                                 0.01   \n",
       "dominance                                   -0.05   \n",
       "familiarity                                 -0.01   \n",
       "frequency                                   -0.02   \n",
       "iconicity/transparency                      -0.05   \n",
       "imageability                                -0.00   \n",
       "motor                                       -0.01   \n",
       "naming                                      -0.06   \n",
       "number of features                          -0.02   \n",
       "recognition memory                          -0.02   \n",
       "semantic decision                           -0.05   \n",
       "semantic diversity                          -0.01   \n",
       "semantic neighborhood                       -0.01   \n",
       "sensory                                     -0.02   \n",
       "social/moral                                -0.04   \n",
       "space/time/quantity                         -0.04   \n",
       "valence                                     -0.03   \n",
       "visual lexical decision                     -0.01   \n",
       "\n",
       "embed                      fMRI_text_denoise_512d_participant  \n",
       "norm_category                                                  \n",
       "age of acquisition                                      -0.03  \n",
       "arousal                                                 -0.02  \n",
       "auditory lexical decision                               -0.03  \n",
       "concreteness                                            -0.01  \n",
       "dominance                                               -0.02  \n",
       "familiarity                                             -0.01  \n",
       "frequency                                               -0.02  \n",
       "iconicity/transparency                                  -0.06  \n",
       "imageability                                            -0.02  \n",
       "motor                                                   -0.01  \n",
       "naming                                                  -0.06  \n",
       "number of features                                      -0.02  \n",
       "recognition memory                                       0.02  \n",
       "semantic decision                                       -0.05  \n",
       "semantic diversity                                      -0.02  \n",
       "semantic neighborhood                                   -0.01  \n",
       "sensory                                                 -0.01  \n",
       "social/moral                                            -0.03  \n",
       "space/time/quantity                                     -0.04  \n",
       "valence                                                 -0.04  \n",
       "visual lexical decision                                 -0.01  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>embed</th>\n",
       "      <th>fMRI_text_cognival</th>\n",
       "      <th>fMRI_text_cognival_participant</th>\n",
       "      <th>fMRI_text_denoise_128d</th>\n",
       "      <th>fMRI_text_denoise_128d_participant</th>\n",
       "      <th>fMRI_text_denoise_256d</th>\n",
       "      <th>fMRI_text_denoise_256d_participant</th>\n",
       "      <th>fMRI_text_denoise_512d</th>\n",
       "      <th>fMRI_text_denoise_512d_participant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age of acquisition</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arousal</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auditory lexical decision</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concreteness</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dominance</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>familiarity</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iconicity/transparency</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imageability</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motor</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naming</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of features</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognition memory</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic decision</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic diversity</th>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic neighborhood</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensory</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social/moral</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>space/time/quantity</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visual lexical decision</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Finding the top-performing fmri_text_denoise\n",
    "sorted_denoise = results.filter(like='denoise', axis=1).mean().sort_values()\n",
    "sorted_denoise"
   ],
   "id": "a031dafc3a775553"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7c7a5b947a4e640"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Subsetting to only the words in norms\n",
    "to_pull = set(\n",
    "    pd.read_csv('../../data/psychNorms/psychNorms.zip', index_col=0, low_memory=False, compression='zip').index\n",
    ")\n",
    "fMRI_text_cognival = fMRI_text_cognival.loc[fMRI_text_cognival.index.isin(to_pull)].astype(float)\n",
    "fMRI_speech_cognival = fMRI_speech_cognival.loc[fMRI_speech_cognival.index.isin(to_pull)].astype(float)\n",
    "fMRI_text_denoise = None\n",
    "\n",
    "# Saving \n",
    "fMRI_text_cognival.to_csv('../../data/embeds/fMRI_text_cognival.csv')\n",
    "fMRI_speech_cognival.to_csv('../../data/embeds/fMRI_speech_cognival.csv')\n",
    "fMRI_text_denoise.to_csv('../../data/embeds/fMRI_text_denoise.csv')"
   ],
   "id": "fe9fc85ee929a493",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
